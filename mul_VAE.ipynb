{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mul-VAE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSHEjH_3wltS",
        "outputId": "2a172cfe-acfe-4747-b2ce-9e3fbbe5245f"
      },
      "source": [
        "!pip install httplib2==0.15.0\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import  pandas as pd\n",
        "import os\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "import seaborn as sn\n",
        "sn.set()\n",
        "\n",
        "import sys\n",
        "import warnings; \n",
        "warnings.simplefilter('ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting httplib2==0.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/83/5e006e25403871ffbbf587c7aa4650158c947d46e89f2d50dcaf018464de/httplib2-0.15.0-py3-none-any.whl (94kB)\n",
            "\r\u001b[K     |███▌                            | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 20kB 18.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 30kB 14.9MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 40kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 51kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 61kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 71kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 81kB 11.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 92kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 6.7MB/s \n",
            "\u001b[?25hInstalling collected packages: httplib2\n",
            "  Found existing installation: httplib2 0.17.4\n",
            "    Uninstalling httplib2-0.17.4:\n",
            "      Successfully uninstalled httplib2-0.17.4\n",
            "Successfully installed httplib2-0.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S6DOfRWwr6x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvN7hfC7wt84"
      },
      "source": [
        "# Drive ID\n",
        "ml_20m_train = '1-3RvqzVCj9dI7Np1qwg7EAlt3RY0eYtJ'\n",
        "ml_20m_val = '1-5Y4TomaQyhXjTcOJHpQEIDl2gbo2cC3'\n",
        "ml_20m_test = '1-8F6ychbApdNtVJQp20ecz1sXEVVDr4I'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZAV-IVTxTvK"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLrnsBNIxWWH"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HVLepmsxXVj"
      },
      "source": [
        "# Download data\n",
        "downloaded = drive.CreateFile({'id':ml_20m_train})   \n",
        "downloaded.GetContentFile('train.csv') \n",
        "\n",
        "downloaded = drive.CreateFile({'id':ml_20m_val})   \n",
        "downloaded.GetContentFile('val.csv') \n",
        "\n",
        "downloaded = drive.CreateFile({'id':ml_20m_test})   \n",
        "downloaded.GetContentFile('test.csv') \n",
        "\n",
        "\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn0fyP3bd92T"
      },
      "source": [
        "fid = '1ywXP4zdEU_r59HMIlEVhTlBGFijCHVK6'\n",
        "http = drive.auth.Get_Http_Object()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gstv-GKzc-NU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cALZe2Ed9Pd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g14vfDgRxx-I"
      },
      "source": [
        "train_data = pd.read_csv('train.csv')\n",
        "val_data = pd.read_csv('val.csv')\n",
        "test_data = pd.read_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88SnMRTWx8Ul"
      },
      "source": [
        "nItems = train_data.sid.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHIBjCmmyoMk",
        "outputId": "ff6b8ead-cde0-47d6-ee0e-a0e9129de6a0"
      },
      "source": [
        "nItems"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19209"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBV4-LWxxV1o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ArwYmycx9qP"
      },
      "source": [
        "train_data = sparse.csr_matrix((np.ones_like(train_data.uid), (train_data.uid.values, train_data.sid.values)), \n",
        "                             dtype='float64',\n",
        "                             shape=(train_data.uid.nunique(),nItems))\n",
        "\n",
        "\n",
        "val_data = sparse.csr_matrix((np.ones_like(val_data.uid), (val_data.uid.values, val_data.sid.values)), \n",
        "                             dtype='float64',\n",
        "                             shape=(val_data.uid.nunique(), nItems))\n",
        "\n",
        "test_data = sparse.csr_matrix((np.ones_like(test_data.uid), (test_data.uid.values, test_data.sid.values)), \n",
        "                             dtype='float64',\n",
        "                             shape=(test_data.uid.nunique(), nItems))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HNCZsytyEv-"
      },
      "source": [
        "class netflixDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, scr_matrix, eval = False,prop=0.2):\n",
        "        self.scr_matrix = scr_matrix\n",
        "        self.eval = eval\n",
        "        self.prop = prop\n",
        "      \n",
        "          \n",
        "    def __getitem__(self, idx):\n",
        "      \n",
        "      item = {}\n",
        "        \n",
        "      \n",
        "\n",
        "      if self.eval:\n",
        "        u_items = self.scr_matrix[idx,:].toarray()[0]\n",
        "        \n",
        "        nu_items = u_items.sum()       \n",
        "        val_size = int(nu_items*self.prop)\n",
        "        idx_labels = np.where(u_items == 1)[0]\n",
        "        data = np.ones_like(u_items)\n",
        "        \n",
        "        \n",
        "                \n",
        "        val_idx = np.random.choice(idx_labels, size=val_size, replace=False)                   \n",
        "        data[val_idx] = 0\n",
        "         \n",
        "        \n",
        "        \n",
        "        \n",
        "        item['data'] = torch.tensor(u_items*data,dtype=torch.float64)     \n",
        "        \n",
        "        item['ground_truth'] = torch.tensor(np.logical_not(data),dtype=torch.float64)             \n",
        "        \n",
        "        \n",
        "       \n",
        "      else:\n",
        "        item['data'] = torch.tensor(self.scr_matrix[idx,:].toarray(),dtype=torch.float64)\n",
        "      return item\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.scr_matrix.shape[0]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyK1Mvfp3Nac"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self,n_Items, hidden=600, dimz= 200, p=0.5):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.n_Items = n_Items\n",
        "        self.dimz = dimz\n",
        "        self.hidden = hidden\n",
        "        self.p = p\n",
        "\n",
        "        self.inference = nn.Sequential(\n",
        "           \n",
        "            nn.Dropout(self.p),\n",
        "            nn.Linear(self.n_Items,self.hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.hidden,2*self.dimz)          \n",
        "        )\n",
        "        self.generative = nn.Sequential(\n",
        "            nn.Linear(self.dimz,self.hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hidden,self.n_Items),\n",
        "            \n",
        "        )\n",
        "  \n",
        "        \n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        \n",
        "        return mu + std*eps* ( 0 if self.Mode =='train' else 1)\n",
        "\n",
        "\n",
        "    def forward(self, x,Mode='train'):       \n",
        "        self.Mode = Mode\n",
        "        x = F.normalize(x, p=2, dim=1)  \n",
        "        distribution = self.inference(x)\n",
        "\n",
        "\n",
        "\n",
        "        mu, logvar = distribution[:, :self.dimz], distribution[:, self.dimz:]\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        logit = self.generative(z)\n",
        "\n",
        "        \n",
        "        return logit, mu, logvar\n",
        "\n",
        "\n",
        "       \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlrmPDOY3QF7"
      },
      "source": [
        "def loss_function(recon_x, x, mu, logvar,N_i,beta):\n",
        "\n",
        "   # BCE = F.binary_cross_entropy(recon_x.view(-1,n_items), x.view(-1,n_items), reduction='sum')\n",
        "    \n",
        "    LL = -torch.mean(torch.sum(F.log_softmax(recon_x, -1) * x, -1))\n",
        "    # see Appendix B from VAE paper:\n",
        "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "    # https://arxiv.org/abs/1312.6114\n",
        "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "   \n",
        "    return LL + beta*KLD "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFeidQp13TCN"
      },
      "source": [
        "\n",
        "def NDCG_at_k(labels, scores, k = 100):\n",
        "  device = scores.device\n",
        "  arg_sort_scores = torch.argsort(scores,1,descending=True)\n",
        "  arg_sort_labels = torch.argsort(labels,1,descending=True)\n",
        "  \n",
        "\n",
        "  pred_labels = torch.gather(labels,1,arg_sort_scores[:,:k]).to(device)\n",
        " \n",
        "\n",
        "  tp = (1. /torch.log(torch.arange(2,2+k))).to(device)\n",
        "  \n",
        " \n",
        "  dcg = (tp * pred_labels).sum(axis = 1)\n",
        " \n",
        "  idcg = torch.Tensor([tp[:min(int(n),k)].sum() for n in labels.sum(1)]).to(device)\n",
        "  \n",
        "  ndcg = (dcg/idcg).mean()\n",
        "\n",
        "  return ndcg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHVbM5e04ji3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O80E8ANc3jWC"
      },
      "source": [
        "# Declare Model\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = VAE(nItems,hidden=1200,dimz=400).to(device)\n",
        "n_Epochs = 200\n",
        "\n",
        "\n",
        "# KL-Annealing\n",
        "anneal = 0\n",
        "anneal_cap = 0.2\n",
        "anneal_steps = 1.0/200_000\n",
        "\n",
        "# prepare Data\n",
        "train_ds = netflixDataset(train_data)\n",
        "train_dl = DataLoader(train_ds, batch_size=512)\n",
        "\n",
        "val_ds = netflixDataset(val_data,eval=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=1024)\n",
        "\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3,weight_decay=0.01)\n",
        "\n",
        "path = 'model51.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_h-eOYH32nk"
      },
      "source": [
        "total_loss = []\n",
        "total_ndcgs = []\n",
        "cur_metric = -np.inf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U6BsElDj35MS"
      },
      "source": [
        "pbar = tqdm(range(n_Epochs),total = n_Epochs)\n",
        "for epoch in pbar:\n",
        "  NDCGs = []\n",
        "  Recalls = []\n",
        "  model.train()\n",
        "  train_loss =  []\n",
        "  metrics = {}\n",
        "  \n",
        "\n",
        "  train_phase = tqdm(enumerate(train_dl),total = len(train_dl),leave = False)\n",
        "  for batch_idx, data in train_phase:\n",
        "      \n",
        "      x = data['data'].float().to(device)\n",
        "      x = x.squeeze(1)\n",
        "      optimizer.zero_grad()  \n",
        "     \n",
        "      \n",
        "      recon_x, mu, logvar = model(x)   \n",
        "     \n",
        "      CE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
        "      KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
        "     \n",
        "      loss =  CE + anneal * KLD\n",
        "      \n",
        "      # loss = loss_function(recon_batch, x, mu, logvar,n_items,anneal)   \n",
        "      anneal = min(anneal+anneal_steps,anneal_cap)   \n",
        "      loss.backward()      \n",
        "      \n",
        "      \n",
        "      optimizer.step()\n",
        "      train_loss.append(loss.item())\n",
        "      metrics['loss'] =  loss.item()\n",
        "      train_phase.set_postfix(metrics)\n",
        "    \n",
        "  \n",
        "  model.eval()\n",
        "  eval_phase = tqdm(enumerate(val_dl),total = len(val_dl),leave = False)\n",
        "\n",
        "  for batch_idx,data in eval_phase:\n",
        "    \n",
        "      \n",
        "    X = data['data'].float().to(device)  \n",
        "    X = X.squeeze(1)\n",
        "    \n",
        "\n",
        "    ground_truth = torch.stack([data['ground_truth'][i,:] for i in range(X.shape[0])])\\\n",
        "                  .squeeze(1).to(device)\n",
        "    \n",
        "   \n",
        "\n",
        "    pred ,_,_= model(X,Mode ='eval')\n",
        "\n",
        "    pred = pred.detach()\n",
        "   \n",
        "    \n",
        "    pred[X!=0] = -np.inf\n",
        "    ndcg = NDCG_at_k(ground_truth,pred)\n",
        "    metrics['loss'] = np.mean(train_loss)\n",
        "    metrics['ndcg'] = ndcg.item()\n",
        "    NDCGs.append(metrics['ndcg'])\n",
        "    eval_phase.set_postfix(metrics)\n",
        "    \n",
        "  \n",
        "  metrics['ndcg'] = np.mean(NDCGs)\n",
        "  total_loss.append(metrics['loss'])\n",
        "  total_ndcgs.append(metrics['ndcg'])\n",
        "  if total_ndcgs[-1] > cur_metric:\n",
        "    cur_metric = total_ndcgs[-1]\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': total_loss[-1],\n",
        "            'ndcg': total_ndcgs[-1],\n",
        "            'beta': anneal\n",
        "            }, path)\n",
        "    checkpoint = drive.CreateFile({\"parents\": [{\"kind\": \"drive#fileLink\", \"id\": fid}]})\n",
        "    checkpoint.SetContentFile(path)\n",
        "    checkpoint.Upload()\n",
        "\n",
        "  pbar.set_postfix(metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jpsbb4wcmxw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q0wGYtU36Hl"
      },
      "source": [
        "plt.figure(figsize=(12, 3))\n",
        "plt.plot(total_loss)\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOYZDyEGgH0E"
      },
      "source": [
        "plt.figure(figsize=(12, 3))\n",
        "plt.plot(total_ndcgs)\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9OVUVBTgdGJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO3ggYAigtzP"
      },
      "source": [
        "\n",
        "model = VAE(nItems)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3,weight_decay=0.01)\n",
        "\n",
        "checkpoint = torch.load(path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "anneal = checkpoint['beta']\n",
        "ndcg = checkpoint['ndcg']\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec4B4u7ng2ar"
      },
      "source": [
        "test_ds = netflixDataset(test_data,eval=True)\n",
        "test_dl = DataLoader(test_ds,batch_size=1024)\n",
        "model.eval()\n",
        "metrics = {}\n",
        "eval_phase = tqdm(enumerate(test_dl),total = len(test_dl),leave = False)\n",
        "NDCGs = []\n",
        "for batch_idx,data in eval_phase:\n",
        "  \n",
        "    \n",
        "  X = data['data'].float().to(device)  \n",
        "  X = X.squeeze(1)\n",
        "  \n",
        "\n",
        "  ground_truth = torch.stack([data['ground_truth'][i,:] for i in range(X.shape[0])])\\\n",
        "            .squeeze(1).to(device)\n",
        "  \n",
        "  \n",
        "\n",
        "  pred ,_,_= model(X,Mode ='eval')\n",
        "\n",
        "  pred = pred.detach()\n",
        "  \n",
        "  \n",
        "  pred[X!=0] = -np.inf\n",
        "  ndcg = NDCG_at_k(ground_truth,pred)\n",
        "\n",
        "  metrics['ndcg'] = ndcg.item()\n",
        "  NDCGs.append(metrics['ndcg'])\n",
        "  eval_phase.set_postfix(metrics)\n",
        "print(np.mean(NDCGs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmHLuvdOg6An"
      },
      "source": [
        "epoch,loss,anneal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i_3pWBUSLz8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}